<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Test Audio</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      background-color: #f5f5f5;
      margin: 0;
      padding: 0;
    }
    .container {
      max-width: 600px;
      margin: 40px auto;
      padding: 30px;
      background-color: white;
      border-radius: 8px;
      box-shadow: 0 2px 10px rgba(0,0,0,0.1);
    }
    h1 {
      text-align: center;
      color: #333;
    }
    .instruction {
      font-size: 16px;
      text-align: center;
      margin-top: 20px;
      color: #555;
    }
    .sentence {
      font-size: 20px;
      text-align: center;
      margin: 15px 0;
      font-weight: bold;
    }
    .tip {
      display: flex;
      align-items: center;
      background-color: #e7f3fe;
      padding: 10px;
      border-left: 4px solid #2196F3;
      border-radius: 4px;
      margin: 10px 0;
      font-size: 14px;
    }
    .tip-icon {
      margin-right: 10px;
      font-weight: bold;
      color: #2196F3;
    }
    button {
      background-color: #e74c3c;
      color: white;
      padding: 10px 15px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
      font-size: 16px;
      width: 100%;
      margin-top: 20px;
    }
    button:hover {
      background-color: #c0392b;
    }
    .status {
      margin-top: 20px;
      padding: 10px;
      border-radius: 4px;
      background-color: #f8f9fa;
      border: 1px solid #e9ecef;
      text-align: center;
    }
    #audioPlayback {
      width: 100%;
      margin-top: 15px;
      display: none;
    }
    .modal {
      display: none;
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background-color: rgba(0,0,0,0.5);
      align-items: center;
      justify-content: center;
    }
    .modal-content {
      background-color: white;
      padding: 20px;
      border-radius: 8px;
      max-width: 400px;
      text-align: center;
    }
    .modal-button {
      background-color: #e74c3c;
      color: white;
      padding: 8px 16px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
      margin-top: 10px;
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>Test Audio</h1>

    <div class="instruction">Please read the sentence aloud into your microphone:</div>
    <div class="sentence">"How is the weather today"</div>

    <div class="tip">
      <div class="tip-icon">ðŸ’¡</div>
      <div>Speak slowly and clearly for AI to recognize the speech</div>
    </div>

    <button id="recordBtn">Record</button>
    <audio id="audioPlayback" controls></audio>

    <div class="status" id="status" data-transcript="{{ transcript }}" data-match="{{ match }}">
      {% if transcript %}
      You said: "{{ transcript }}"
      {% if match %}
      <br><strong>Correct!</strong>
      {% endif %}
      {% endif %}
    </div>
  </div>

  <div class="modal" id="failModal">
    <div class="modal-content">
      <h2>Failed</h2>
      <p>Your spoken text didn't match the target sentence. Please try again.</p>
      <button class="modal-button" id="closeModalBtn">OK</button>
    </div>
  </div>

  <script>
    let isRecording = false;
    let mediaRecorder;
    let recordedChunks = [];
    let audioStream;

    const recordBtn = document.getElementById('recordBtn');
    const statusDiv = document.getElementById('status');
    const audioPlayback = document.getElementById('audioPlayback');
    const failModal = document.getElementById('failModal');
    const closeModalBtn = document.getElementById('closeModalBtn');
    const testSentence = "how is the weather today";

    recordBtn.addEventListener('click', async () => {
      if (!isRecording) {
        startRecording();
      } else {
        stopRecording();
      }
    });

    closeModalBtn.addEventListener('click', () => {
      failModal.style.display = 'none';
    });

    async function startRecording() {
      try {
        audioStream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
            sampleRate: 44100,
            channelCount: 1
          }
        });
        
        mediaRecorder = new MediaRecorder(audioStream);
        recordedChunks = [];

        mediaRecorder.ondataavailable = e => {
          if (e.data.size > 0) recordedChunks.push(e.data);
        };

        mediaRecorder.onstop = async () => {
          const audioBlob = new Blob(recordedChunks, { type: 'audio/webm' });
          const audioUrl = URL.createObjectURL(audioBlob);
          
          // Check if audioPlayback exists before setting properties
          if (audioPlayback) {
            audioPlayback.src = audioUrl;
            audioPlayback.style.display = 'block';
          }
          
          // Convert to WAV
          const arrayBuffer = await audioBlob.arrayBuffer();
          const audioContext = new AudioContext();
          const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
          const wavBlob = encodeWAV(audioBuffer);
          
          // Send to backend
          const formData = new FormData();
          formData.append("audio", wavBlob, "recording.wav");
          
          try {
            const response = await fetch("/test-audio/", {
              method: "POST",
              headers: {
                "Accept": "application/json"
              },
              body: formData
            });
            console.log("Response status:", response);
            
            const data = await response.json();
            console.log("Response data:", data);
            
            const spoken = data.transcript?.toLowerCase().trim();
            statusDiv.textContent = `You said: "${spoken}"`;
            console.log("Spoken text:", spoken);
            
            if (data.match) {
              statusDiv.innerHTML += "<br><strong>Correct!</strong>";
              recordBtn.textContent = 'Next';
              recordBtn.onclick = () => window.location.href = '/interview/';
            } else {
              failModal.style.display = 'flex';
              recordBtn.textContent = 'Record';
            }
          } catch (error) {
            statusDiv.textContent = 'Error processing audio: ' + error.message;
            recordBtn.textContent = 'Try Again';
          }
          
          // Stop all tracks
          audioStream.getTracks().forEach(track => track.stop());
        };

        mediaRecorder.start();
        isRecording = true;
        recordBtn.textContent = 'Stop Recording';
        statusDiv.textContent = 'Recording...';
        if (audioPlayback) {
          audioPlayback.style.display = 'none';
        }
      } catch (error) {
        statusDiv.textContent = 'Error accessing microphone: ' + error.message;
      }
    }

    function stopRecording() {
      if (mediaRecorder && mediaRecorder.state !== 'inactive') {
        mediaRecorder.stop();
        isRecording = false;
        statusDiv.textContent = 'Processing...';
      }
    }

    function encodeWAV(audioBuffer) {
      const numChannels = audioBuffer.numberOfChannels;
      const sampleRate = audioBuffer.sampleRate;
      const format = 1; // PCM
      const bitDepth = 16;
  
      const samples = mergeBuffers(audioBuffer);
      const dataSize = samples.length * 2; // 16-bit = 2 bytes per sample
  
      const buffer = new ArrayBuffer(44 + dataSize);
      const view = new DataView(buffer);
  
      function writeString(view, offset, string) {
        for (let i = 0; i < string.length; i++) {
          view.setUint8(offset + i, string.charCodeAt(i));
        }
      }
  
      // RIFF chunk descriptor
      writeString(view, 0, 'RIFF');
      view.setUint32(4, 36 + dataSize, true);
      writeString(view, 8, 'WAVE');
  
      // fmt sub-chunk
      writeString(view, 12, 'fmt ');
      view.setUint32(16, 16, true); // Subchunk1Size (16 for PCM)
      view.setUint16(20, format, true); // AudioFormat
      view.setUint16(22, numChannels, true);
      view.setUint32(24, sampleRate, true);
      view.setUint32(28, sampleRate * numChannels * 2, true); // ByteRate
      view.setUint16(32, numChannels * 2, true); // BlockAlign
      view.setUint16(34, bitDepth, true);
  
      // data sub-chunk
      writeString(view, 36, 'data');
      view.setUint32(40, dataSize, true);
  
      // Write samples
      let offset = 44;
      for (let i = 0; i < samples.length; i++) {
        const s = Math.max(-1, Math.min(1, samples[i]));
        view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
        offset += 2;
      }
  
      return new Blob([view], { type: 'audio/wav' });
    }
  
    function mergeBuffers(audioBuffer) {
      const channels = [];
      for (let i = 0; i < audioBuffer.numberOfChannels; i++) {
        channels.push(audioBuffer.getChannelData(i));
      }
  
      const length = audioBuffer.length;
      let result;
      
      if (audioBuffer.numberOfChannels === 1) {
        // Mono
        result = new Float32Array(length);
        result.set(channels[0]);
      } else {
        // Stereo or more - need to interleave
        result = new Float32Array(length * audioBuffer.numberOfChannels);
        for (let i = 0; i < length; i++) {
          for (let channel = 0; channel < audioBuffer.numberOfChannels; channel++) {
            result[i * audioBuffer.numberOfChannels + channel] = channels[channel][i];
          }
        }
      }
  
      return result;
    }
  </script>
</body>
</html>